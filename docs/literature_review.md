

| No. | Paper Title & Link | Venue | Year | Type | TL;DR of the paper | Why it’s useful | Use | Priority |
| :---- | :---- | :---- | :---- | ----- | :---- | :---- | ----- | ----- |
| 1 | Improving LLM-as-a-Judge Inference with the Judgment Distribution [https://arxiv.org/abs/2503.03064](https://arxiv.org/abs/2503.03064) | EMNLP 2025 Findings | 2025 | Neither | The paper argues that when you use an LLM as a “judge” (to evaluate or rank text), it’s better not to just pick its single most likely judgment (the mode), but to **use the full probability distribution** over judgments (e.g. take the mean). Doing so yields more accurate and fine-grained evaluations, especially when you also incorporate risk-sensitive methods. Interestingly, using chain-of-thought reasoning can narrow (collapse) that distribution and actually hurt the LLM’s judging performance.  |  | N/A | Must read |
| 2 | Judging LLM as a judge [https://arxiv.org/abs/2306.05685](https://arxiv.org/abs/2306.05685) | NeurIPS | 2023 | Neither | The authors explore whether strong LLMs can reliably *judge* other LLMs’ outputs, especially for open-ended conversations. They identify biases (verbosity, self-promotion, positional effects) that can distort judgments. To validate trustworthiness, they introduce two benchmarks — **MT-Bench** (multi-turn dialogues) and **Chatbot Arena** (crowdsourced comparisons) — and show that GPT-4, acting as judge, matches human preference judgments over 80% of the time. Their conclusion: “LLM-as-judge” is a viable, scalable proxy for human evaluations in many scenarios. |  | N/A | Good to know |
| 3 | LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks [https://aclanthology.org/2025.acl-short.20.pdf](https://aclanthology.org/2025.acl-short.20.pdf) | ACL | 2025 | Neither | The authors introduce **JUDGE-BENCH**, a benchmark of 20 NLP datasets covering varied tasks (translation, dialogue, toxicity, coherence, etc.), each with human judgments, and test 11 different LLMs (open-source and proprietary) to see how well the LLMs replicate human judgments. They find that while some models align well with human evaluators on certain tasks (like instruction following or reasoning traces), performance is inconsistent across properties, datasets, and whether the text is human- or machine-generated. Chain-of-Thought prompting doesn’t reliably help, and no single LLM is superior in all contexts. The authors warn: before using an LLM as a judge/evaluator, always validate it against human judgments for *that specific task*. |  | N/A | Must read |
| 4 | Beyond Monoliths: Expert Orchestration for More Capable, Democratic, and Safe Large Language Models [https://arxiv.org/pdf/2506.00051](https://arxiv.org/pdf/2506.00051) |  | 2025 | Canonical | The authors argue that instead of pushing ever larger “generalist” LLMs, we should shift to a **modular, orchestrated system** of specialist models. In their “expert orchestration” framework, **judges** evaluate models on dimensions like factuality or ethics, and **routers** direct user queries to the most appropriate specialist(s). This approach promises higher performance, more transparency, better safety, and democratization of model development by letting domain experts build niche models without needing to match a giant universal model. |  | N/A | Must read |
| 5 | UltraFeedback: Boosting Language Models with Scaled AI Feedback [https://arxiv.org/abs/2310.01377](https://arxiv.org/abs/2310.01377) | ICML | 2024 | Neither | They propose generating a **massive AI-feedback dataset** (1M+ annotations from GPT-4 over 250K user–assistant conversations) to overcome the scarcity of human feedback in aligning LMs. [arxiv.org](https://arxiv.org/abs/2310.01377) Using this “UltraFeedback,” they train a LLaMA-based model via best-of-n sampling and reinforcement learning, showing that scaled AI feedback can meaningfully improve performance on chat benchmarks. |  | N/A | Skippable |
| 6 | Closing the Evaluation Gap: Ensembling LLM-Judges Generates More Reliable Inference-Time Reference-Free Critiques[https://openreview.net/forum?id=LcqSSpFXcb](https://openreview.net/forum?id=LcqSSpFXcb) | ACL ARR | 2024 | Neither | A single LLM used as a judge (i.e. generating critiques without a reference solution) is theoretically and empirically insufficient to produce optimal feedback. Instead, the authors show that **ensembling multiple LLM-judges** (i.e. multiple critique‐generating calls) reduces a “suboptimality gap” and yields better performance in downstream tasks (e.g. code generation via prompt optimization). Empirically, their ensemble approach achieves up to \~9% more solved problems compared to single-judge baselines. |  | N/A | Good to know |
| 7 | Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation[https://arxiv.org/pdf/2507.21028](https://arxiv.org/pdf/2507.21028) |  | 2025 | Specific | They introduce **MAJ-EVAL**, a framework where multiple LLM agents—each with a distinct “persona” (i.e. representing different evaluation criteria or perspectives)—debate and critique candidate outputs. These evaluator agents are not arbitrarily defined but are **automatically constructed** from domain texts (e.g. research papers). MAJ-EVAL generates multi-dimensional feedback, and the authors show it agrees more closely with expert human judgments (in domains like education and medicine) than traditional metrics or single-judge LLM methods. |  | N/A | Must read |
| 8 | Can Large Language Models Be an Alternative to Human Evaluations? [https://aclanthology.org/2023.acl-long.870/](https://aclanthology.org/2023.acl-long.870/) | Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics  | 2023 | Neither | They test whether LLMs can stand in for humans in textual evaluation tasks: give LLMs exactly the same prompts, examples, and evaluation questions that human judges see, and ask the LLMs to rate or critique outputs. They find that (i) LLM judgments align quite well with expert human judgments on tasks like story generation and adversarial attacks, (ii) LLM evaluations are robust to formatting changes and sampling strategies, and (iii) while promising, there remain limitations and ethical concerns before fully replacing human evaluation. |  | N/A | Skippable |
| 9 | Snorkel: Rapid Training Data Creation with Weak Supervision [https://arxiv.org/abs/1711.10160](https://arxiv.org/abs/1711.10160) | Proceedings of the VLDB Endowment | 2017 | Neither | Instead of painstakingly hand-labeling massive datasets, the authors propose a system called **Snorkel** in which domain experts write *labeling functions*—heuristics, patterns, weak models—that noisily label data. Snorkel then uses a generative model to denoise and integrate these weak signals (estimating accuracies, correlations) without needing ground truth labels, producing probabilistic labels. Those labels feed into a downstream discriminative model. In experiments and user studies, Snorkel accelerates training data creation (users build models \~2.8× faster) and attains performance close (within a few percent) to fully hand-labeled datasets, while outperforming simpler heuristic baselines.  |  | N/A | Good to know |
| 10 | Aligning AI With Shared Human Values [https://openreview.net/forum?id=dNy\_RKzJacY](https://openreview.net/forum?id=dNy_RKzJacY) | ICLR | 2021 | Neither | They introduce **ETHICS**, a benchmark spanning justice, well-being, duties, virtues, and common-sense morality, aimed at testing whether language models understand and can apply basic moral principles in varied scenarios. They show that current LMs have **some** ability to predict human moral judgments, but it’s far from perfect. The paper argues this is a promising step toward embedding ethical reasoning and value alignment in AI systems. |  | N/A | Good to know |
| 11 | ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate[https://arxiv.org/abs/2308.07201](https://arxiv.org/abs/2308.07201) | ICLR | 2024 | Neither | They propose **ChatEval**, a system where multiple LLM agents (a “referee team”) debate among themselves to evaluate generated text (rather than relying on a single model‐judge). The idea is that each agent brings a different perspective, and the debate yields a more human-like, reliable assessment. Empirically, ChatEval shows better alignment with human judgments on open‐ended generation tasks than single‐agent evaluators.  |  | N/A | Skippable |
| 12 | AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors[https://openreview.net/forum?id=EHg5GDnyq1](https://openreview.net/forum?id=EHg5GDnyq1) | ICLR | 2024 | Neither | AgentVerse is a framework to coordinate multiple expert LLM agents to work together collaboratively, rather than relying on a single monolithic agent. The authors show through experiments (in text understanding, coding, tool use, embodied tasks) that AgentVerse’s multi-agent teams outperform single agents. They also analyze emergent behaviors among agents (how roles, communication patterns, division of labor arise) and argue that this kind of orchestration unlocks more than just “sum of parts” performance.  |  | N/A | Skippable |
| 13 | A Little Human Data Goes A Long Way [https://arxiv.org/abs/2410.13098](https://arxiv.org/abs/2410.13098) | ACL | 2025 | Neither | They examine how much synthetic (machine-generated) training data can replace human-annotated data in *fact verification* (FV) and *evidence-based QA*. Up to \~90% of the data can be synthetic with only small performance loss, but going beyond that (i.e. fully synthetic) causes a sharp drop. Remarkably, adding just \~125 human-generated examples to a synthetic dataset substantially recovers performance. They also analyze cost tradeoffs, showing that achieving the same boost via synthetic data demands an order of magnitude more synthetic points than using modest human annotation.  |  | N/A | Must read |
| 14 | Constitutional AI: Harmlessness from AI Feedback [https://arxiv.org/pdf/2212.08073](https://arxiv.org/pdf/2212.08073) | ANTHROPIC | 2022 | Neither | They propose **Constitutional AI (CAI)**, a method to train AI assistants that are both helpful **and** harmless, without relying on human labeling of harmfulness. Instead, the system uses a “constitution” — a set of principles — to let the AI critique and revise its own outputs (supervised phase), and then uses **reinforcement learning from AI feedback** (RLAIF) to further refine the model’s behavior. The result is an AI that handles harmful queries by objecting or explaining, rather than evading or being unsafe, with reduced reliance on human feedback. |  | N/A | Skippable |
| 15 | FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance [https://arxiv.org/abs/2305.05176](https://arxiv.org/abs/2305.05176) | TMLR | 2024 | Neither | They observe that different LLM APIs have wildly different cost-structures (sometimes orders of magnitude apart) and propose strategies to use them more efficiently. Their framework, **FrugalGPT**, combines three cost-saving ideas—**prompt adaptation**, **LLM approximation**, and especially **LLM cascade** (choosing cheaper models for easy queries, reserving more powerful ones for hard queries). In experiments, FrugalGPT can match GPT-4’s performance while reducing cost by up to 98%, or improve accuracy by \~4% at the same cost. |  | N/A | Good to know |
| 16 | ChatGPT outperforms crowd workers for text-annotation tasks [https://www.pnas.org/doi/10.1073/pnas.2305016120](https://www.pnas.org/doi/10.1073/pnas.2305016120) | PNAS(?) | 2023 | Neither | They test ChatGPT on several annotation tasks (relevance, stance, topic, frame detection) in four datasets (tweets, news) with \~6,183 items, comparing it against crowd workers and trained annotators. ChatGPT’s zero-shot accuracy is on average \~25 percentage points higher than crowd workers, and its inter-coder agreement (consistency) exceeds that of both crowd workers and trained annotators across tasks. Moreover, the cost per annotation using ChatGPT is extremely low (less than $0.003), making it \~30× cheaper than MTurk. |  | N/A | Good to know |
| 17 | RouteLLM: Learning to Route LLMs with Preference Data[https://arxiv.org/pdf/2406.18665](https://arxiv.org/pdf/2406.18665) | ICLR | 2025 | Specific | They propose a *router model* that, for each user query, dynamically decides whether to call a “strong” (expensive, high-quality) or “weak” (cheaper, lower-quality) LLM. The router is trained using human preference data (and augmented data) to predict which model will likely give better output. Evaluations show that this routing scheme can cut costs by more than half while preserving most of the strong model’s response quality. |  | Related Work | Must read |
| 18 | AutoMix: Automatically Mixing Language Models [https://arxiv.org/abs/2310.12963](https://arxiv.org/abs/2310.12963) | NeurIPS | 2024 | Neither | They propose **AutoMix**, a framework that dynamically routes queries between smaller and larger LLMs (accessible only via black-box APIs), depending on how confident the smaller model is in its answer. The key innovations are a *self-verification* step (where the small model estimates whether its answer is reliable) and a partially observable Markov decision process (POMDP) router that mitigates noise in the verification. Empirically, across reasoning datasets, AutoMix reduces computational cost by over 50% while maintaining performance comparable to always using the larger model |  | N/A | Good to know |
| 19 | The rise and potential of large language model based agents: a survey [https://link.springer.com/article/10.1007/s11432-024-4222-0](https://link.springer.com/article/10.1007/s11432-024-4222-0) |  | 2023 | Neither | This is a comprehensive survey of the emerging field of **LLM-based agents**. The authors propose a unified conceptual framework (dividing agent architecture into *brain, perception,* and *action* modules), review how such agents are constructed (e.g. with tool use, memory, reasoning), catalog many application domains (single agents, multi-agent systems, human-agent interaction), examine “agent societies,” and highlight key challenges and open research directions (e.g. evaluation, safety, scaling, interpretability) |  | N/A | Skippable |
| 20 | When AIs Judge AIs: The Rise of Agent-as-a-Judge Evaluation for LLMs[https://arxiv.org/abs/2508.02994](https://arxiv.org/abs/2508.02994) |  | 2025 | Neither | As LLMs grow more capable and autonomous, evaluating them becomes harder—especially for open-ended, multi-step tasks. Yu argues for a shift from “LLM as judge” to **“agent as judge”**, where the evaluator itself is an agent (with memory, tool use, reasoning steps) that can assess another agent’s entire reasoning and actions, not just final outputs. The paper traces the evolution from single-model judges → multi-agent debates → agentic evaluators, compares their trade-offs (cost, reliability, alignment with humans), surveys domain applications (medicine, law, education), and outlines looming challenges like bias, robustness, and how to trust the judges themselves.  | Perhaps use this if we want to expand into chain-of-thought(?) | N/A | Good to know |
| 21 | LLM-based Automated Grading with Human-in-the-Loop [https://arxiv.org/abs/2504.05239](https://arxiv.org/abs/2504.05239) |  | 2025 | Specific | They propose **GradeHITL**, a framework that augments automatic short-answer grading (ASAG) by having an LLM dynamically ask human experts clarifying questions to refine rubrics and interpretations. This human-in-the-loop interaction reduces errors and helps the system approach human-level grading performance, especially in rubric-based evaluation settings where fully automated methods struggle. | Could help making better judges. | N/A | Good to know |
| 22 | Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates [https://arxiv.org/abs/2408.13006](https://arxiv.org/abs/2408.13006) | Building Trust in LLMs and LLM Applications workshop at ICLR | 2025 | Neither | The paper rigorously examines how well LLMs serve as “judges” in alignment tasks (e.g. ranking summaries or assistance outputs), focusing on two underexplored issues: the interpretability of reliability metrics and the effect of prompt templates. They define new, more explainable metrics and introduce methods to mitigate internal inconsistency of LLM judges. Their experimental results show (i) prompt design significantly impacts the judge’s reliability, and (ii) the alignment between LLM judges and human evaluators is only moderate, not perfect |  | Related Work | Must read |
| 23 | Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges [https://arxiv.org/pdf/2508.00454](https://arxiv.org/pdf/2508.00454) |  | 2025 | Specific | The paper addresses the inefficiency and biases of “LLM-as-judge” approaches for evaluating multi-turn dialogues by proposing **MTDEval**, a lightweight evaluator that absorbs the judgments of multiple LLMs into a single model. They build a dataset (P2-MTD) of pairwise preferences annotated by several judges and train the evaluator to predict ratings or rankings. MTDEval preserves the benefits of ensemble judgment but drastically cuts inference cost, and empirically outperforms prior baselines across multiple dialogue evaluation benchmarks in both accuracy and robustness. |  | Related Work | Skippable |
| 24 | Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning [https://arxiv.org/abs/2310.03094](https://arxiv.org/abs/2310.03094) | ICLR | 2024 | Canonical | They propose a **cascade strategy** to reduce cost in reasoning tasks: use a cheaper LLM (e.g. GPT-3.5) for “easy” questions, and only escalate to a more powerful (and expensive) LLM (e.g. GPT-4) when necessary. They decide “when to escalate” by checking *answer consistency* (i.e. how stable the cheaper model’s answers are under sampling). To improve that consistency check, they use a **mixture of thought representations** (combining chain-of-thought and program-of-thought) to better estimate uncertainty. Empirically, on six reasoning benchmarks, their approach achieves similar accuracy to always using the expensive model, but uses only \~40% of its cost. | Alternative to LLM routing, older. | N/A | Good to know |


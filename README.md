# Multi-Judge Interpretability

Framework for approximating human preferences using multiple specialized judges and learned aggregation.

Developed for the [Apart x Martian Mechanistic Router Interpretability Hackathon](https://apartresearch.com/sprints/apart-x-martian-mechanistic-router-interpretability-hackathon-2025-05-30-to-2025-06-01) where it won 2nd place. Published to Neurips both LatinX, and Reliable ML from Unreliable Data workshops.

## Key Features

- **10 Specialized Judges**: Harmlessness, factual accuracy, helpfulness, conciseness, and more
- **Learned Aggregation**: GAM and MLP models that combine judge scores to match human preferences
- **Interpretability**: Analyze which judges matter most for human preferences
- **Robustness**: Test aggregator performance under contaminated data and rubric variations

## Prerequisites

- Python 3.10+
- PyTorch
- Martian API access (for judge creation)
- OpenAI/Lambda AI API (for human feedback simulation)

## Setup

```bash
# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your API keys
```

## Main Dataset

The primary dataset `dataset/data_with_judge_scores.pkl` (2000 samples) must be generated by running the full pipeline. It contains:
- Questions and answers from UltraFeedback
- Scores from 10 specialized judges
- Human feedback from 8 simulated personas

To generate the dataset, follow the development workflow in CLAUDE.md or run:
```bash
python run_full_experiment.py --data-source ultrafeedback --data-size 2000
```

## Running Experiments

### Full Experiment Pipeline

```bash
# Run complete experiment (judge scoring + persona simulation + model training)
python run_full_experiment.py --data-source ultrafeedback --data-size 2000
```

### Post-Hoc Analysis

Add GAM analysis and baseline comparisons to existing results:

```bash
# GAM hyperparameter tuning + baseline comparisons
python analyze_existing_experiment.py --experiment-dir results/full_experiments/main_experiment_results

# Stability analysis of GAM features
python gam_stability_analysis.py --experiment-dir results/full_experiments/main_experiment_results --n-runs 20
```

## Repository Structure

```
judge-aggregator/
├── analysis/                    # Analysis library code
│   ├── gam_hyperparameter_tuning.py
│   ├── mlp_hyperparameter_tuning.py
│   ├── correlation_analysis.py
│   └── run_correlation_analysis.py
├── experiments/                 # Research experiments
│   ├── persona_poisoning/       # Robustness to contaminated training data
│   ├── rubric_sensitivity/      # Sensitivity to rubric variations
│   └── aggregator_validation/   # Validation on low-variance targets
├── pipeline/                    # Core pipeline components
│   ├── core/                    # Judge evaluation, training, baselines
│   └── utils/                   # Judge rubrics, data utilities
├── dataset/                     # Main dataset (2000 samples)
├── results/full_experiments/    # Experiment results
│   └── main_experiment_results/ # Primary experiment results
├── config/                      # Training configurations
├── run_full_experiment.py       # Main experiment runner
├── analyze_existing_experiment.py  # Post-hoc GAM analysis
└── gam_stability_analysis.py    # Feature stability analysis
```

## Experiments

Three experiments validate different aspects:

1. **Persona Poisoning** - Tests robustness to contaminated human feedback (random noise, systematic bias, scale compression)
2. **Rubric Sensitivity** - Measures stability across semantic variations of judge rubrics
3. **Aggregator Validation** - Validates performance on low-variance ground truth (UltraFeedback overall_score, single personas)

Each experiment has its own README in `experiments/`.
# Multi-Judge Interpretability

Framework for approximating human preferences using multiple specialized judges and learned aggregation.

Developed for the [Apart x Martian Mechanistic Router Interpretability Hackathon](https://apartresearch.com/sprints/apart-x-martian-mechanistic-router-interpretability-hackathon-2025-05-30-to-2025-06-01) where it won 2nd place. Published to Neurips both LatinX, and Reliable ML from Unreliable Data workshops.

## Key Features

- **10 Specialized Judges**: Harmlessness, factual accuracy, helpfulness, conciseness, and more
- **Learned Aggregation**: GAM and MLP models that combine judge scores to match human preferences
- **Interpretability**: Analyze which judges matter most for human preferences
- **Robustness**: Test aggregator performance under contaminated data and rubric variations

## Prerequisites

- Python 3.10+
- PyTorch
- Martian API access (for judge creation)
- OpenAI/Lambda AI API (for human feedback simulation)

## Setup

```bash
# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your API keys
```

**Note**: External datasets (JUDGE-BENCH, MSLR) are included in the repository. StorySparkQA auto-downloads from HuggingFace when needed.

## Dataset Setup

This repository uses multiple datasets for different experiments:

### Workshop Dataset (Legacy)
- **Location**: `dataset/data_with_judge_scores.pkl`
- **Source**: UltraFeedback with 8 synthetic personas
- **Size**: 2000 samples
- **Usage**: Workshop experiments (persona poisoning, rubric sensitivity, aggregator validation)

### Fellowship Datasets

#### 1. JUDGE-BENCH (Human Annotations)
- **Location**: `dataset/judge-bench/`
- **Source**: [dmg-illc/JUDGE-BENCH](https://github.com/dmg-illc/JUDGE-BENCH)
- **Contains**: 19 diverse NLP evaluation tasks with human annotations
- **Usage**: Track 1.3 (baseline performance), Track 2.2 (cross-task analysis)
- **Setup**: Already cloned (manual)

#### 2. MSLR Annotated Dataset (Medical Summarization)
- **Location**: `dataset/mslr-annotated/`
- **Source**: [allenai/mslr-annotated-dataset](https://github.com/allenai/mslr-annotated-dataset) (commit: 3317358, 2023-05-18)
- **Contains**: Human-annotated medical literature summaries with facet judgments (470 reviews)
- **Key File**: `dataset/mslr-annotated/data/data_with_overlap_scores.json` (5.5MB)
- **Usage**: Track 1.2 (MAJ-Eval comparison)
- **Setup**: Manually downloaded, version tracked in `dataset/mslr-annotated/VERSION`

#### 3. StorySparkQA (Long-form QA)
- **Source**: [NEU-HAI/StorySparkQA](https://huggingface.co/datasets/NEU-HAI/StorySparkQA)
- **Setup**: Auto-downloaded by `dataset_loader.py` when needed
- **Usage**: Track 1.2 (MAJ-Eval comparison)

### Dataset Structure
```
dataset/
├── data_with_judge_scores.pkl          # Workshop data (UltraFeedback + personas)
├── judge-bench/                        # JUDGE-BENCH (19 NLP tasks, manually cloned)
│   ├── README.md
│   ├── cola/, dices/, llmbar/, ...
│   └── [19 task directories]
└── mslr-annotated/                     # MSLR (manually downloaded)
    ├── data/
    │   └── data_with_overlap_scores.json  # 5.5MB, 470 reviews
    ├── LICENSE                         # Apache 2.0
    ├── README.md                       # Dataset documentation
    └── VERSION                         # Version tracking (commit: 3317358)
```

## Main Dataset

The primary dataset `dataset/data_with_judge_scores.pkl` (2000 samples) must be generated by running the full pipeline. It contains:
- Questions and answers from UltraFeedback
- Scores from 10 specialized judges
- Human feedback from 8 simulated personas

To generate the dataset, follow the development workflow in CLAUDE.md or run:
```bash
python run_full_experiment.py --data-source ultrafeedback --data-size 2000
```

## Running Experiments

### Full Experiment Pipeline

```bash
# Run complete experiment (judge scoring + persona simulation + model training)
python run_full_experiment.py --data-source ultrafeedback --data-size 2000
```

### Post-Hoc Analysis

Add GAM analysis and baseline comparisons to existing results:

```bash
# GAM hyperparameter tuning + baseline comparisons
python analyze_existing_experiment.py --experiment-dir results/full_experiments/main_experiment_results

# Stability analysis of GAM features
python gam_stability_analysis.py --experiment-dir results/full_experiments/main_experiment_results --n-runs 20
```

## Repository Structure

```
judge-aggregator/
├── analysis/                    # Analysis library code
│   ├── gam_hyperparameter_tuning.py
│   ├── mlp_hyperparameter_tuning.py
│   ├── correlation_analysis.py
│   └── run_correlation_analysis.py
├── experiments/                 # Research experiments
│   ├── persona_poisoning/       # Robustness to contaminated training data
│   ├── rubric_sensitivity/      # Sensitivity to rubric variations
│   └── aggregator_validation/   # Validation on low-variance targets
├── pipeline/                    # Core pipeline components
│   ├── core/                    # Judge evaluation, training, baselines
│   └── utils/                   # Judge rubrics, data utilities
├── dataset/                     # Main dataset (2000 samples)
├── results/full_experiments/    # Experiment results
│   └── main_experiment_results/ # Primary experiment results
├── config/                      # Training configurations
├── run_full_experiment.py       # Main experiment runner
├── analyze_existing_experiment.py  # Post-hoc GAM analysis
└── gam_stability_analysis.py    # Feature stability analysis
```

## Experiments

Three experiments validate different aspects:

1. **Persona Poisoning** - Tests robustness to contaminated human feedback (random noise, systematic bias, scale compression)
2. **Rubric Sensitivity** - Measures stability across semantic variations of judge rubrics
3. **Aggregator Validation** - Validates performance on low-variance ground truth (UltraFeedback overall_score, single personas)

Each experiment has its own README in `experiments/`.